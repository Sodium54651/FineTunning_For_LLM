from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextIteratorStreamer
import torch, os, threading
print("–ó–∞–≥—Ä—É–∂–∞—é—Å—å..")
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
try:
    curdir = os.path.dirname(os.path.abspath(__file__))
    model_name = curdir + r"\FluffleMothLearned"

    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ
    # quant_config = BitsAndBytesConfig(load_in_8bit=True)
    # model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config, device_map="cpu")
    # model = AutoModelForCausalLM.from_pretrained("gpt2")  # –∏–ª–∏ –¥—Ä—É–≥–∞—è –±–∞–∑–æ–≤–∞—è –∞—Ä—Ö-—Ä–∞

    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cpu")
except Exception as ex:
    print("üõë–û—à–∏–±–æ—á–∫–∞", ex)
    exit



dialogue = ""
print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –ú–æ–∂–Ω–æ –æ–±—â–∞—Ç—å—Å—è.")
while True:
    # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    user_input = input("\n–¢—ã: ")

    with open(f"{curdir}\Logs.txt", 'a', encoding='utf-8') as fin:
        dialogue += f"User: {user_input}\n"
        fin.write(f"?User: {user_input}\n\n")

    if user_input.lower() in ["–≤—ã—Ö–æ–¥", "exit", "quit"]:
        break

    

    inputs = tokenizer(dialogue, return_tensors="pt", truncation=True, max_length=512)

    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    thread = threading.Thread(
        target=model.generate,
        kwargs=dict(
            **inputs,                     # —Ç–æ–∫–µ–Ω—ã –∏ attention mask
            streamer=streamer,            # —Å—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞
            max_new_tokens=128,           # –º–∞–∫—Å. –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞, –±–æ–ª—å—à–µ ‚Üí –¥–ª–∏–Ω–Ω–µ–µ –æ—Ç–≤–µ—Ç
            min_length=1,                # –º–∏–Ω. –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞, –±–æ–ª—å—à–µ ‚Üí –æ—Ç–≤–µ—Ç –¥–ª–∏–Ω–Ω–µ–µ
            do_sample=True,               # —Å–ª—É—á–∞–π–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è, True ‚Üí –±–æ–ª—å—à–µ –∫—Ä–µ–∞—Ç–∏–≤–∞
            top_k=50,                      # –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Ç–æ–ø‚ÄëK, –±–æ–ª—å—à–µ ‚Üí –±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
            top_p=0.9,                     # nucleus sampling, –±–æ–ª—å—à–µ ‚Üí –±–æ–ª–µ–µ —Å–ª—É—á–∞–π–Ω—ã–π –æ—Ç–≤–µ—Ç
            temperature=0.8,               # —Ç–≤–æ—Ä—á–µ—Å–∫–æ—Å—Ç—å, –±–æ–ª—å—à–µ ‚Üí –∫—Ä–µ–∞—Ç–∏–≤–Ω–µ–µ
            repetition_penalty=1.2,       # —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä, –±–æ–ª—å—à–µ ‚Üí –º–µ–Ω—å—à–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
            no_repeat_ngram_size=2,       # –∑–∞–ø—Ä–µ—â–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ 2‚Äë—Å–ª–æ–≤–Ω—ã—Ö —Ñ—Ä–∞–∑, –±–æ–ª—å—à–µ ‚Üí –º–µ–Ω—å—à–µ –ø–æ–≤—Ç–æ—Ä–æ–≤
            length_penalty=1.0,           # —à—Ç—Ä–∞—Ñ –∑–∞ –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞, –±–æ–ª—å—à–µ ‚Üí –¥–ª–∏–Ω–Ω–µ–µ –æ—Ç–≤–µ—Ç—ã
            num_return_sequences=1        # —Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ –≤–µ—Ä–Ω—É—Ç—å
    )
    )

    thread.start()

    print("\nü§ñ:", end="", flush=True)
    full_reply = ""
    for new_text in streamer:
        print(new_text, end="", flush=True)
        full_reply += new_text
    thread.join()

    dialogue += f"AI: {full_reply}\n"
    with open(f"{curdir}\Logs.txt", 'a', encoding='utf-8') as fin:
        fin.write(f"!AI: {full_reply}\n\n")
    print()
    # print("\n–¥–∏–æ–ª–æ–≥–∏" + dialogue)
