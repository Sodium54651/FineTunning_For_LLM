import os
print("–ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫...")
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

from transformers import AutoTokenizer, Trainer, TrainingArguments
from transformers import AutoModelForCausalLM as AMFS1 #1 —Ç–∏–ø –±–∏–±–ª–∏—Ç–µ–∫
from transformers import AutoModelForSeq2SeqLM as AMFS2  #2 —Ç–∏–ø –±–∏–±–ª–∏–æ—Ç–µ–∫
from datasets import load_dataset
import torch












    
# —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def test_loss():
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_trainer = Trainer(
        model=model,
        args=training_args,
        eval_dataset=tokenized_test_datasets["test"]
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
    # metrics –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, –≤–∫–ª—é—á–∞—è loss
    metrics = test_trainer.evaluate()
    
    # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ loss –∏–∑ –º–µ—Ç—Ä–∏–∫
    test_loss = metrics['eval_loss']
    
    # –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ loss
    print(f"üìä  –¢–µ–∫—É—â–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å loss: {test_loss}")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ loss –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    return test_loss





# üßä–±–ª–æ–∫ –≤—ã–±–æ—Ä–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞

# –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ cpu or cuda
svich_dev = "cuda" if torch.cuda.is_available() else "cpu"
# –∑–∞–¥–∞—ë–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –≤–µ—â —á–∏—Å–µ–ª —Ç–æ–ª—å–∫–æ –¥–ª—è cuda —è–¥–µ—Ä
if svich_dev == "cuda":
    fp = True
else:
    fp = False











# üßä–±–ª–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö

print("–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—á–∞–ª–∞—Å—å...")
# –ø—Ä–æ–ø–∏—Å–∞–Ω–∏–µ –ø—É—Ç–µ–π –∫ –¥–∞–Ω–Ω—ã–º –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ
epoch = 3
testYes = True
tokensSize = 256      # 512 ~ 3000 —Å–ª–æ–≤ –∏ –∑–Ω–∞–∫–æ–≤ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤

curdir = os.path.dirname(os.path.abspath(__file__))
model_path = curdir + r"\QA_Inator"          #–º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ–º–∞—è
data_path = curdir + r"\QA_Inator_data.txt"     #–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
test_path = curdir + r"\QA_Inator_test.txt"     #–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω
save_path = curdir + r"\QA_Inator_learned"  #–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å









# üßä–±–ª–æ–∫ —Ç–æ–∫–∏–Ω–µ–∑–∞—Ü–∏–∏

print("üìù  –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
# —Ç–æ–∫–∏–Ω–∏–∑–∞—Ü–∏—è –º—ã –±–µ—Ä—ë–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –º–æ–¥–µ–ª–∏ –ø–æ–ø—É—Ç–∏ —á—Ç–æ –ø—Ä–æ–ø–∏—Å–∞–ª–∏ –≤—ã—à–µ, –ø—É—Å—Ç—å —Ç–∞–º –∏—â–µ—Ç
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
print("üíæ  –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
# model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)
try:
    model = AMFS1.from_pretrained(model_path, local_files_only=True)
    print("‚ö†Ô∏è  –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 1 —Ç–∏–ø –±–∏–±–ª–∏–æ—Ç–µ–∫")
except Exception:
    print("‚ö†Ô∏è  —Å–º–µ–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–∞ 2 —Ç–∏–ø:")
    model = AMFS2.from_pretrained(model_path, local_files_only=True)
model.to(svich_dev)
print("‚öôÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: ", model.device)


#‚¨áÔ∏è –ø–æ–¥–±–ª–æ–∫ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö

with open(data_path, "r", encoding="utf-8") as f:
    lines = f.readlines()
    lines = [line.strip() for line in lines if line.strip()]
    # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ
    big_line = 0
    for line in lines:
        encoded = tokenizer(line, add_special_tokens=False)
        tokens = len(encoded["input_ids"])
        if big_line < tokens:
            big_line = tokens

    totalTokens = big_line
    tokensSize = totalTokens + 10

print(f"üìä  –°—Ä–µ–¥–Ω–µ–µ –ø–æ –±–æ–ª—å–Ω–∏—Ü–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {totalTokens}")
print(f"üìè  –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –≤—ã–±—Ä–∞–Ω: {tokensSize}")

# ‚¨áÔ∏è–ø–æ–¥ –±–ª–æ–∫ —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –∏ –µ–≥–æ —Ç–æ–∫–∏–Ω–µ–∑–∞—Ü–∏—è

print("üóÑÔ∏è  –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
# –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á—Ç–æ –º—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏
dataset = load_dataset("text", data_files={"train": data_path})
try:
    testdataset = load_dataset("text", data_files={"test": test_path})
except Exception as e:
    print("‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:", e)
    testYes = False

# —Ñ—É–Ω–∫—Ü–∏—è —á—Ç–æ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
def tokenize(text):
    model_inputs = tokenizer(text["text"], truncation=True, padding="max_length", max_length=tokensSize)
    # labels –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—Ç–æ, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å)
    # —Ç—É—Ç –Ω–∞–¥–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç –æ–Ω–∞ –∂–¥—ë—Ç 
    try:
        # —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –≤–æ–ø—Ä–æ—Å –∏ —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –æ—Ç–≤–µ—Ç
        model_inputs["labels"] = tokenizer(text["text"], truncation=True, padding="max_length", max_length=tokensSize)["input_ids"]
    except Exception as e:
        # —ç—Ç–∞ —Å—Ç—Ä–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ 1 —Å—Ç—Ä–æ—á–∫—É –∏ —Ö–≤–∞—Ç–∏—Ç
        model_inputs["labels"] = model_inputs["input_ids"].copy()
    return model_inputs

print("üìùüóÑÔ∏è  –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞...")
tokenized_datasets = dataset.map(tokenize, batched=True)
tokenized_datasets.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
# –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
if testYes:
    print("üìùüóÑÔ∏è  –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    tokenized_test_datasets = testdataset.map(tokenize, batched=True)
    tokenized_test_datasets.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

    












# üßä–ø–æ–¥–±–æ—Ä –ª—É—á—à–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

# fp –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ cuda —è–¥—Ä–∞
if fp:
    print("üõ¢Ô∏è  –í—ã–ø–æ–Ω—è–µ—Ç—Å—è –ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –±–∞—á–µ–π")
    best_batch = None
    for b in [128, 64, 32, 16, 8, 4, 2, 1]:
        print(f"üîé  –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—á–∞ {b}")
        try:
        # –≤—ã–±–æ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            training_args = TrainingArguments(
                output_dir=save_path,
                overwrite_output_dir=True,
                per_device_train_batch_size=b,
                fp16=fp,
                save_strategy="no",
                logging_strategy="no",
                dataloader_num_workers=0,
                max_steps=3
            )
        # –¥–µ–ª–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ç–µ—Å—Ç ‚Äî –ø–∞—Ä—É —à–∞–≥–æ–≤

            trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_datasets["train"],
            )

            trainer.train()
            best_batch = b
            print(f"‚úÖ  –ü–æ–¥—Ö–æ–¥–∏—Ç batch_size={b}")
            break

        except RuntimeError as e:
            if "CUDA out of memory" in str(e):
                print(f"‚ö†Ô∏è  batch_size={b} –Ω–µ –≤–ª–µ–∑, —É–º–µ–Ω—å—à–∞–µ–º...")
                torch.cuda.empty_cache()
            else:
                raise e

    if best_batch is None:
        best_batch = 1  # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
    print(f"üèÅ  –§–∏–Ω–∞–ª—å–Ω—ã–π batch_size={best_batch}")

    training_args = TrainingArguments(
        output_dir=save_path,
        overwrite_output_dir=True,
        per_device_train_batch_size=best_batch,
        fp16=fp,
        save_strategy="no",
        logging_strategy="steps",
        logging_steps=500,
        dataloader_num_workers=0,
    )


# –µ—Å–ª–∏ —É —Ç–µ–±—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∏ 16 –≥–± RAM
else:
    #‚öôÔ∏è  –∑–∞–¥–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    training_args = TrainingArguments(
        output_dir=save_path,
        overwrite_output_dir=True,
        # num_train_epochs=3,
    # —Ä–∞–∑–º–µ—Ä –∫—É—Å—è, —ç—Ç–æ –∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∑–∞ 1 –ø—Ä–∏—Å–µ—Å—Ç –æ–±—É—á–∞–ª–∫–∞ –±—É–¥–µ—Ç –±—Ä–∞—Ç—å –≤–æ—Ç —Å—Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ—á–µ–∫
    # –∏ –Ω–∞ —ç—Ç–∏—Ö —Å—Ç—Ä–æ—á–∫–∞—Ö –¥–µ–±–∞—Ç—å backward, –∏ –∫–∞–∂–¥–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è –≤ 2 —Ä–∞–∑–∞
    # –Ω–æ –∏ –ø–∞–º—è—Ç—å —Ç–æ–∂–µ –Ω–∞–≥—Ä—É–∂–∞–µ—Ç –≤ 2 —Ä–∞–∑–∞, –∞ —Ç–∞–∫ –∂–µ —á–µ–º –±–æ–ª—å—à–µ –±–∞—Ç—á —Ç–µ–º –ª—É—á—à–µ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å
    # batch –±–æ–ª—å—à–µ = –±–æ–ª—å—à–µ RAM –∏ –ª—É—á—à–µ —É—Å–≤–∞–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
        per_device_train_batch_size=2,
    # –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–µ 
        fp16=fp,
    # –º–µ—Ç–æ–¥—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        save_strategy="no",
        logging_strategy="steps",
    # –≥–æ–≤–æ—Ä–∏—Ç —á–µ—Ä–µ–∑ —Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç —à–∞–≥
        logging_steps=10,
        dataloader_num_workers=0  # —ç—Ç–∞ —à—Ç—É–∫–∞ –¥–ª—è –≤–∏–Ω–¥—ã –¥–æ–ª–∂–∞ –±—ã—Ç—å –æ–Ω–∞ –¥–µ–ª–∞–µ—Ç —Ö–æ—Ä–æ—à–æ
    )
    print(f"üèÅ  –§–∏–Ω–∞–ª—å–Ω—ã–π batch_size={2}")

# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞, —É–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏, –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
)











# üßä–±–ª–æ–∫ –∑–∞–ø—É—Å–∫–∞

print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
try:
    for rh in range(epoch):
        trainer.train()
        if testYes:
            lert = test_loss()
            if  lert < 1.5:
                print(f"{epoch}, {current_loss}")
                print("üèÜ  –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–æ—Å—Ä–æ—á–Ω–æ!")
                break
        else:
            if len(trainer.state.log_history) > 0:
                last_log = trainer.state.log_history[-1]
                if "loss" in last_log:
                    current_loss = last_log["loss"]
                    if current_loss < 1.0:
                        print(f"{epoch}, {current_loss}")
                        print("üèÜ  –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–æ—Å—Ä–æ—á–Ω–æ!")
                        break

    print("–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    print("–°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å")
    trainer.save_model(save_path)
    tokenizer.save_pretrained(save_path)
    print("üü© –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    input("‚ö° –ú–æ–¥–µ–ª—å –æ–±—É—á–∏–ª–∞—Å—å –Ω–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É...")
except KeyboardInterrupt:
    print("\nüü• –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –≤—Ä—É—á–Ω—É—é. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
    trainer.save_model(save_path)
    tokenizer.save_pretrained(save_path)
    print("üü© –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    # input("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞—Å—å, –Ω–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É...")